# Recent ML Research

Created: `2021-05-25`

I've been collecting articles to read -- or that I have read and want to hold on to -- as github stars, browser favorites, saved reddit posts.... 

I used to collect them in a table in a github gist, and then was using arxiv-sanity.org...

Let's try this and see what happens.


# To Review

## Architecture/theory

* FNet
  * Fourier transforms replacing attention in transformers
  * Q: Could this be used to accelerate pretraining?  
* [2019 - Preetum Nakkiran - "More Data Can Hurt for Linear Regression: Sample-wise Double Descent"](https://arxiv.org/abs/1912.07242)
  * [Gradient Double Descent](../topics/gradient-double-descent.md) in linear regression
 

## Causal Inference


[Parent Category](../)

<!-- TAGS
-->
