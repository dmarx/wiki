# Recent ML Research

Created: `2021-05-25`

I've been collecting articles to read -- or that I have read and want to hold on to -- as github stars, browser favorites, saved reddit posts.... 

I used to collect them in a table in a github gist, and then was using arxiv-sanity.org...

Let's try this and see what happens.


# To Review

## Architecture/theory

### Misc

* ["More Data Can Hurt for Linear Regression: Sample-wise Double Descent" - 2019 - Preetum Nakkiran -](https://arxiv.org/abs/1912.07242)
  * [Gradient Double Descent](../topics/gradient-double-descent.md) in linear regression

### Transformers

* FNet - ["FNet: Mixing Tokens with Fourier Transforms" - 2021 - James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon](https://arxiv.org/abs/2105.03824  )
  * Google Research
  * Yannic Kilcher overview: https://www.youtube.com/watch?v=JJR3pBl78zw&t=1297s 
  * Fourier transforms replacing attention in transformers
  * Q: Could this be used to accelerate pretraining?  

* ExpireSpan - ["Not All Memories are Created Equal: Learning to Forget by Expiring"](https://arxiv.org/abs/2105.06548)
  * Facebook AI
  * https://github.com/facebookresearch/transformer-sequential
  * Yannic Kilcher: https://www.youtube.com/watch?v=2PYLNHqxd5A 

## Causal Inference


<!-- TAGS
-->
