# Recent ML Research

Created: `2021-05-25`

I've been collecting articles to read -- or that I have read and want to hold on to -- as github stars, browser favorites, saved reddit posts.... 

I used to collect them in a table in a github gist, and then was using arxiv-sanity.org...

Let's try this and see what happens.


# To Review

## Architecture/theory

* ["FNet: Mixing Tokens with Fourier Transforms" - 2021 - James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon](https://arxiv.org/abs/2105.03824  )
  * Google Research
  * Yannic Kilcher overview: https://www.youtube.com/watch?v=JJR3pBl78zw&t=1297s 
  * Fourier transforms replacing attention in transformers
  * Q: Could this be used to accelerate pretraining?  
* ["More Data Can Hurt for Linear Regression: Sample-wise Double Descent" - 2019 - Preetum Nakkiran -](https://arxiv.org/abs/1912.07242)
  * [Gradient Double Descent](../topics/gradient-double-descent.md) in linear regression
 

## Causal Inference


[Parent Category](../)

<!-- TAGS
-->
